# LLM-RAG-AGENTS
This project is a modular project focused on the independent exploration of key technologies in LLM development. The project includes:

Building LLMs from Scratch: Creating large language models from the ground up, with a focus on understanding architectural design and tokenization strategies.

Training Optimization Techniques: Applying advanced techniques to reduce GPU VRAM usage and speed up training, such as gradient checkpointing, mixed precision, FlashAttention, and efficient batching.

Reinforcement Learning for LLMs: Implementing fine-tuning strategies based on reinforcement learning (e.g., PPO, DPO) to improve alignment and model behavior.

Using Modern Training Repositories: Exploring and guiding the use of modern, open-source LLM training frameworks such as HuggingFace Transformers, DeepSpeed, and NanoGPT.

RAG (Retrieval-Augmented Generation): Implementing standalone RAG systems to improve factual accuracy by combining retrieval with generation.

Autonomous Agents: Developing agent-based architectures that can perform multi-step tasks, access tools, and reason over long sequences of actions independently.

Each component is designed as an independent module to allow focused learning and experimentation.
<br><br>

## 1. Building LLMs from Scratch

### 1.1 Origin Transformer

<br><br>

### 1.2 ...

<br><br>

## 2. Training Optimization Techniques

### 1.1 ...

<br><br>

### 1.2 ...

<br><br>

## 3. Reinforcement Learning for LLMs

### 1.1 ...

<br><br>

### 1.2 ...

<br><br>

## 4. Using Modern Training Repositories

### 1.1 ...

<br><br>

### 1.2 ...

<br><br>

## 5. RAG (Retrieval-Augmented Generation)

### 1.1 ...

<br><br>

### 1.2 ...

<br><br>

## 6. Autonomous Agents

### 1.1 ...

<br><br>

### 1.2 ...

<br><br>

## 7. Reference
- [Attention is All You Need, 2017 - Google](https://arxiv.org/abs/1706.03762)

<br><br>
